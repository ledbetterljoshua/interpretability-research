/* ============================================================
   DATA â€” All concept/detail panel content, separated from logic
   ============================================================ */

/** Overview detail panel data (methods on the overview page) */
export const overviewDetails = {
  sae: {
    title: 'Sparse Autoencoders (SAEs)',
    tag: 'tag-method',
    tagText: 'Method',
    desc: `SAEs are the primary tool for "cracking open" superposition. They take the messy, polysemantic activations from a neural network layer and decompose them into a much larger set of sparse, interpretable features.`,
    details: [
      'Train an autoencoder with a sparsity penalty on the hidden layer',
      'Hidden layer is much wider than input (e.g., 16x or 64x)',
      'Each hidden unit ideally corresponds to one interpretable concept',
      'The "Golden Gate Bridge" feature, "code is buggy" feature, etc.',
      'Current standard: TopK or BatchTopK activation, not L1 regularization',
      'Key limitation: different random seeds find different features',
    ],
    prereqs: 'Linear algebra, basic autoencoders, understanding of transformer residual stream',
  },
  circuits: {
    title: 'Circuit Discovery',
    tag: 'tag-method',
    tagText: 'Method',
    desc: 'Circuits are subnetworks within a model that implement specific computations. Finding circuits means identifying which components (attention heads, MLP neurons, features) work together to produce a behavior.',
    details: [
      'Example: "induction heads" are a 2-head circuit that implements in-context copying',
      'Found by activation patching: swap activations and see what breaks',
      'Attribution patching: faster approximation using gradients',
      'Circuits can span many layers and involve complex interactions',
      'Current frontier: attribution graphs automate circuit discovery',
    ],
    prereqs: 'Transformer internals, activation patching technique, understanding of attention',
  },
  attribution: {
    title: 'Attribution Graphs',
    tag: 'tag-method',
    tagText: 'Method (2025)',
    desc: 'Anthropic\'s latest breakthrough. Attribution graphs trace the full computational path from input to output, showing which features activate and how they influence each other. Like a call graph for neural computation.',
    details: [
      'Uses "replacement models" with interpretable components',
      'Maps how features at each layer influence features at subsequent layers',
      'Revealed: Claude plans rhymes ahead when writing poetry',
      'Revealed: Claude solves math differently than it claims in chain-of-thought',
      'Open-sourced: Circuit Tracer library, Neuronpedia frontend',
      'Works on Gemma-2-2B and Llama-3.1-1B (open weights models)',
    ],
    prereqs: 'SAE features, basic circuit concepts, transformer architecture',
  },
  steering: {
    title: 'Representation Steering',
    tag: 'tag-method',
    tagText: 'Method',
    desc: 'Once you find meaningful directions in a model\'s representation space, you can push activations along those directions at inference time to change behavior. No retraining needed.',
    details: [
      'Compute a "steering vector" by contrasting activations on positive vs negative examples',
      'Add the vector to residual stream activations during inference',
      'Can increase honesty, change personality, control topic focus',
      'Goodfire built a commercial API around this',
      'Advanced: Conceptors (ellipsoidal regions), conditional steering (CAST)',
      'Limitation: global static vectors don\'t adapt to context',
    ],
    prereqs: 'Linear algebra, understanding of residual stream, feature concepts',
  },
  probing: {
    title: 'Linear Probes',
    tag: 'tag-method',
    tagText: 'Method',
    desc: 'Train a simple linear classifier on a model\'s internal activations to test whether specific information is represented there. The simplest interpretability technique.',
    details: [
      'If a linear probe can predict a concept from activations, that concept is linearly represented',
      'Supports the "linear representation hypothesis"',
      'Fast to train, easy to interpret',
      'Limitation: finding a probe works doesn\'t mean the model uses that information',
      'Often used as a first step before deeper analysis',
    ],
    prereqs: 'Basic ML (logistic regression), understanding of hidden states',
  },
};


/** Core concepts detail panel data */
export const conceptDetails = {
  'residual-stream': {
    title: 'Residual Stream',
    tag: 'tag-core', tagText: 'Foundational',
    desc: 'The central "highway" of a transformer. Each token has a residual stream vector that flows through the entire network. Attention heads and MLPs read from and write to this stream. Think of it as shared memory that every component can access.',
    details: [
      'Dimension = model\'s hidden size (e.g., 768 for GPT-2 Small)',
      'Every attention head and MLP layer adds to the residual stream',
      'The final residual stream is projected to vocabulary logits',
      'Key insight: components communicate through the residual stream, not directly',
      'This view (from Elhage et al. 2021) is foundational to all mech interp',
    ],
    prereqs: 'Basic understanding of neural network layers',
  },
  'attention': {
    title: 'Attention Heads',
    tag: 'tag-core', tagText: 'Foundational',
    desc: 'Each attention head computes a pattern of which tokens to attend to, then moves information from attended positions to the current position. They\'re the model\'s way of routing information between tokens.',
    details: [
      'QK circuit: determines what to attend to (query-key dot product)',
      'OV circuit: determines what information to move (output-value projection)',
      'Each head has its own learned QK and OV matrices',
      'Induction heads: a famous 2-head circuit that copies patterns from context',
      'Attention patterns are directly visualizable (and often interpretable)',
    ],
    prereqs: 'Linear algebra, matrix multiplication',
  },
  'mlp': {
    title: 'MLP Layers',
    tag: 'tag-core', tagText: 'Foundational',
    desc: 'Multi-layer perceptrons (MLPs) in transformers are where most "knowledge" and "computation" happens. Each MLP layer reads the residual stream, applies a nonlinear transformation, and writes back. Think of them as the processing units, while attention does routing.',
    details: [
      'Typical structure: Linear up-projection \u2192 activation (GELU/ReLU) \u2192 Linear down-projection',
      'Up-projection goes from hidden_size to 4x hidden_size',
      'Individual MLP neurons can sometimes be interpreted (but often polysemantic)',
      'Key neurons might represent specific knowledge or computations',
      'Transcoders replace MLPs with sparse, interpretable alternatives',
    ],
    prereqs: 'Neural network basics, activation functions',
  },
  'embeddings': {
    title: 'Embeddings',
    tag: 'tag-core', tagText: 'Foundational',
    desc: 'The embedding layer converts tokens (integers) into vectors in the residual stream. These initial vectors encode the model\'s "prior" about each token before any processing.',
    details: [
      'Token embedding: maps token ID \u2192 vector',
      'Position embedding: adds positional information',
      'Together they form the initial residual stream',
      'Embedding space has geometric structure (king - man + woman \u2248 queen)',
    ],
    prereqs: 'Basic understanding of vector spaces',
  },
  'logits': {
    title: 'Logits & Unembedding',
    tag: 'tag-core', tagText: 'Foundational',
    desc: 'The unembedding layer converts the final residual stream vector back into a probability distribution over tokens. The "logit lens" technique applies unembedding at intermediate layers to see what the model "thinks" at each step.',
    details: [
      'Unembedding matrix: residual stream \u2192 vocabulary-sized vector',
      'Softmax converts logits to probabilities',
      'Logit lens: apply unembedding at each layer to watch predictions evolve',
      'Often reveals the model "knowing" the answer before the final layer',
    ],
    prereqs: 'Softmax function, basic probability',
  },
  'superposition': {
    title: 'Superposition',
    tag: 'tag-problem', tagText: 'Core Problem',
    desc: 'The central challenge. Neural networks represent far more features (concepts) than they have dimensions. They exploit the fact that in high-dimensional spaces, you can pack many nearly-orthogonal directions. This means individual neurons don\'t correspond to individual concepts.',
    details: [
      'A 768-dimensional space can fit thousands of nearly-orthogonal directions',
      'The model "compresses" many features into fewer dimensions',
      'Makes individual neurons polysemantic (fire for multiple things)',
      'Toy Models of Superposition (2022): mathematical framework showing when/how this happens',
      'Key finding: superposition is stronger for features that are more sparse (rarely active)',
      'The entire SAE research program exists to address this problem',
    ],
    prereqs: 'Linear algebra, high-dimensional geometry intuitions',
  },
  'polysemanticity': {
    title: 'Polysemanticity',
    tag: 'tag-problem', tagText: 'Core Problem',
    desc: 'The observable symptom of superposition: individual neurons fire for multiple, seemingly unrelated concepts. A single neuron might activate for "cats", "the color blue", and "legal documents." This makes neurons uninterpretable in isolation.',
    details: [
      'Most neurons in large models are polysemantic',
      'Not just noise: the model is using this neuron for all those concepts simultaneously',
      'Can arise from superposition (information packing) or incidentally (regularization, noise)',
      'SAEs aim to decompose polysemantic neurons into monosemantic features',
      'Some neurons ARE monosemantic (rare but exist) \u2014 these are interpretable directly',
    ],
    prereqs: 'Understanding of neurons/activations',
  },
  'feature-concept': {
    title: 'Features (The Real Units)',
    tag: 'tag-core', tagText: 'Key Concept',
    desc: 'In mech interp, a "feature" is a direction in activation space that corresponds to a single, interpretable concept. Features are the real units of meaning in a neural network \u2014 not neurons. SAEs try to find them.',
    details: [
      'Features are directions, not neurons (a feature might use many neurons)',
      'A feature might be "text about the Golden Gate Bridge" or "code has a bug"',
      'Features can be abstract, multilingual, multimodal',
      'Found by SAEs: each SAE latent ideally = one feature',
      'Features can be causally meaningful: amplifying a feature changes behavior',
      'Key debate: are the "true features" even well-defined, or model-dependent?',
    ],
    prereqs: 'Vectors and directions in high-dimensional space',
  },
  'linear-rep': {
    title: 'Linear Representation Hypothesis',
    tag: 'tag-core', tagText: 'Key Hypothesis',
    desc: 'The hypothesis that concepts in neural networks are represented as linear directions in activation space. If true, you can find concepts with linear probes and manipulate them with linear steering. Much of mech interp assumes this.',
    details: [
      'Supported by: linear probes work surprisingly well, steering vectors work',
      'Challenged by: some concepts might be represented nonlinearly',
      'The SAE approach explicitly assumes linear features',
      'If wrong (or only partially right), we need different decomposition methods',
      'Current evidence: works well for many concepts, but not a guarantee',
    ],
    prereqs: 'Linear algebra, concept of linear subspaces',
  },
  'sae-detail': {
    title: 'Sparse Autoencoders (SAEs)',
    tag: 'tag-method', tagText: 'Primary Method',
    desc: 'The workhorse of modern mech interp. An SAE takes dense, polysemantic activations and decomposes them into a sparse set of monosemantic features. Think: a learned dictionary that translates "neuron language" into "concept language."',
    details: [
      'Architecture: encoder (activations \u2192 sparse features) + decoder (features \u2192 reconstructed activations)',
      'Hidden dimension >> input dimension (e.g., 16x\u201364x expansion)',
      'Sparsity enforced via TopK, BatchTopK, or L1 penalty',
      'Each active feature corresponds to a specific concept being present',
      'Trained on large datasets of model activations',
      'Key variants: standard SAE, gated SAE, TopK SAE',
      'Evaluation: reconstruction quality + interpretability of features + downstream task performance',
      'Limitation: features not stable across random seeds',
    ],
    prereqs: 'Autoencoders, sparsity, the superposition problem',
  },
  'activation-patching': {
    title: 'Activation Patching',
    tag: 'tag-method', tagText: 'Method',
    desc: 'The fundamental causal intervention technique. Run the model on two inputs, then swap ("patch") activations from one run into the other at specific points. If the output changes, that component is causally important for the behavior.',
    details: [
      'Also called "causal tracing" or "interchange intervention"',
      'Steps: (1) run on clean input, (2) run on corrupted input, (3) patch clean activations into corrupted run at specific locations',
      'If patching restores correct behavior \u2192 that component is necessary',
      'Can patch at any granularity: layers, heads, neurons, individual positions',
      'More rigorous than just looking at activation magnitudes',
      'Expensive: requires multiple forward passes per patch',
    ],
    prereqs: 'Transformer architecture, concept of causal intervention',
  },
  'attribution-patching': {
    title: 'Attribution Patching',
    tag: 'tag-method', tagText: 'Method',
    desc: 'A faster approximation of activation patching using gradients. Instead of actually swapping activations (expensive), use the gradient to estimate what would happen if you did. Orders of magnitude faster, usually a good approximation.',
    details: [
      'Computes: gradient \u00d7 (clean activation - corrupted activation)',
      'First-order approximation of the causal effect',
      'Enables scanning all components quickly to find important ones',
      'Then use full activation patching on the candidates for verification',
      'Also called "attribution" or "gradient-based attribution"',
    ],
    prereqs: 'Gradients, chain rule, activation patching concept',
  },
  'logit-lens': {
    title: 'Logit Lens',
    tag: 'tag-method', tagText: 'Method',
    desc: 'Apply the model\'s unembedding matrix at intermediate layers to see what the model would predict at each processing step. Reveals how the model\'s "belief" about the next token evolves through layers.',
    details: [
      'Simple idea: at each layer, multiply residual stream by unembedding matrix',
      'Shows the model converging on its answer layer by layer',
      'Often: early layers = broad topic, middle layers = narrowing, final layers = specific token',
      'Variants: "tuned lens" trains a learned probe per layer for better accuracy',
      'Quick diagnostic tool: does the model "know" the answer early on?',
    ],
    prereqs: 'Unembedding, softmax, residual stream',
  },
  'probing-detail': {
    title: 'Linear Probes',
    tag: 'tag-method', tagText: 'Method',
    desc: 'Train a linear classifier on model activations to test if information is linearly represented. The simplest and oldest interpretability technique. If a linear probe can extract a concept, that concept has a direction in activation space.',
    details: [
      'Train: logistic regression on activations \u2192 concept label',
      'If high accuracy \u2192 concept is linearly represented at that layer',
      'Fast, cheap, easy to interpret',
      'Limitation: finding information doesn\'t prove the model uses it',
      'Limitation: a good probe might be learning its own features, not finding the model\'s',
      'Best used as a first pass before deeper investigation',
    ],
    prereqs: 'Logistic regression, basic ML evaluation',
  },
  'ablation': {
    title: 'Ablation Studies',
    tag: 'tag-method', tagText: 'Method',
    desc: 'Remove or zero out specific components and measure the impact on model behavior. The simplest form of causal intervention: break something, see what happens.',
    details: [
      'Zero ablation: set component output to zero',
      'Mean ablation: replace with average activation (often better)',
      'Knockout: remove entire attention heads or MLP layers',
      'Can reveal necessity (the model breaks) but not sufficiency (maybe other paths exist)',
      'Iterative ablation can map out redundancy in the network',
    ],
    prereqs: 'Transformer components, forward pass',
  },
  'crosscoders': {
    title: 'Crosscoders',
    tag: 'tag-method', tagText: 'Frontier 2024\u201325',
    desc: 'Extension of SAEs that read and write across multiple layers (subject to causality constraints). Enable tracking how features evolve through the network and comparing features between different models.',
    details: [
      'Standard SAE: encode/decode at one layer',
      'Crosscoder: encode from one layer, decode to multiple layers (or vice versa)',
      'Model diffing: find features shared between two models, and features unique to each',
      'Cross-architecture diffing: compare Llama vs Qwen (architecturally different!)',
      'Found ideological features: Chinese state narratives in Qwen, American exceptionalism in Llama',
      'Uses BatchTopK activation (not L1) to avoid artifacts',
    ],
    prereqs: 'SAEs, multi-layer transformer processing',
  },
  'transcoders': {
    title: 'Transcoders / Cross-Layer Transcoders (CLTs)',
    tag: 'tag-method', tagText: 'Frontier 2025',
    desc: 'Replace MLP layers entirely with sparse, interpretable alternatives. A CLT reads from the residual stream at one layer and contributes to MLP outputs at all subsequent layers. Dramatically simplifies circuit tracing.',
    details: [
      'Standard transcoder: activations at layer N \u2192 predict MLP output at layer N+1',
      'Cross-layer transcoder: activations at layer N \u2192 contribute to ALL subsequent MLP layers',
      'Can substitute for model\'s MLPs with ~50% output matching',
      'Makes attribution graphs much cleaner (fewer entangled paths)',
      'Key innovation behind Anthropic\'s circuit tracing work',
      'Related: MOLT (Mixture of Linear Transforms)',
    ],
    prereqs: 'SAEs, MLP layers, circuit concepts',
  },
  'attribution-graphs': {
    title: 'Attribution Graphs',
    tag: 'tag-method', tagText: 'Frontier 2025',
    desc: 'Anthropic\'s March 2025 breakthrough. Full computational graphs showing how features at each layer influence features at subsequent layers, ultimately producing the output. Like a profiler or call graph for neural computation.',
    details: [
      'Built using CLTs (cross-layer transcoders) as the interpretable substrate',
      'Each node: a feature activation. Each edge: influence from one feature to another.',
      'Applied to Claude 3.5 Haiku, revealing mechanisms for poetry, math, multilingual processing',
      'Key finding: model plans rhymes ahead of time when writing poetry',
      'Key finding: model\'s actual math computation differs from its chain-of-thought explanation',
      'Open-sourced: Circuit Tracer library for open-weight models',
      'Interactive explorer on Neuronpedia',
    ],
    prereqs: 'SAE features, transcoders/CLTs, transformer architecture',
  },
  'rep-engineering': {
    title: 'Representation Engineering (RepE)',
    tag: 'tag-method', tagText: 'Approach',
    desc: 'A top-down alternative to bottom-up circuit analysis. Instead of finding individual features, find directions in representation space that correspond to high-level concepts like "honesty" or "power-seeking," then read or control them.',
    details: [
      'Two parts: Representation Reading (extract concepts) + Representation Control (steer behavior)',
      'Compute concept direction by contrasting activations on positive vs negative examples',
      'Can read: "is this model being honest right now?" (probe the direction)',
      'Can control: add the direction to activations to increase honesty',
      'Complementary to SAE approach (top-down vs bottom-up)',
      'Dan Hendrycks / CAIS team',
    ],
    prereqs: 'Linear algebra, contrastive learning idea, residual stream',
  },
  'steering-vectors': {
    title: 'Steering Vectors / Control Vectors',
    tag: 'tag-method', tagText: 'Application',
    desc: 'Concrete application of representation engineering: compute a vector that represents a concept, then add it to the model\'s activations at inference time to shift behavior in that direction.',
    details: [
      'Simple version: mean(positive examples) - mean(negative examples) = steering vector',
      'Add \u03b1 \u00d7 steering_vector to residual stream at chosen layer(s)',
      '\u03b1 controls strength (too much = incoherent output)',
      'Advanced: Conceptors (ellipsoidal regions, better than simple addition)',
      'Advanced: CAST (conditional activation steering, triggers only on specific inputs)',
      'Goodfire\'s commercial product is essentially a steering vector API',
    ],
    prereqs: 'Representation engineering concepts, vector arithmetic',
  },
  'model-diffing': {
    title: 'Model Diffing',
    tag: 'tag-method', tagText: 'Frontier 2025',
    desc: 'Using crosscoders to compare the internal representations of different models. Find features they share and features unique to each. Like a diff tool for neural networks.',
    details: [
      'Train a crosscoder on paired activations from two models',
      'Shared features: present in both models (e.g., basic language understanding)',
      'Dedicated features: unique to one model (e.g., ideological biases)',
      'Can compare: base vs fine-tuned, model A vs model B, checkpoint T1 vs T2',
      'Breakthrough: first cross-architecture diff (Llama vs Qwen, 2025)',
      'Found: censorship behaviors, political leanings, training data artifacts',
    ],
    prereqs: 'Crosscoders, SAE features, understanding of model training',
  },
};
