{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Looking Inside GPT-2\n",
    "\n",
    "**Goal:** Develop an intuitive, hands-on understanding of transformer internals — not from reading about them, but from poking at a real model.\n",
    "\n",
    "We'll use **GPT-2 Small** (124M parameters, 12 layers, 12 attention heads per layer, 768-dimensional residual stream) — the \"fruit fly\" of mech interp.\n",
    "\n",
    "By the end of this notebook you'll be able to:\n",
    "1. Load a model and access any internal activation\n",
    "2. Understand the residual stream as the model's \"shared memory\"\n",
    "3. Visualize what attention heads are actually doing\n",
    "4. See how the model's prediction evolves layer by layer (logit lens)\n",
    "5. Do your first causal intervention (what breaks if we remove a component?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer_lens as tl\n",
    "from transformer_lens import HookedTransformer, utils\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "# Check device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"TransformerLens version: {tl.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load GPT-2 Small\n",
    "\n",
    "TransformerLens wraps the model so every internal activation is accessible. First download will take a moment, then it's cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "\n",
    "print(f\"Model: {model.cfg.model_name}\")\n",
    "print(f\"Layers: {model.cfg.n_layers}\")\n",
    "print(f\"Attention heads per layer: {model.cfg.n_heads}\")\n",
    "print(f\"Hidden dimension (residual stream): {model.cfg.d_model}\")\n",
    "print(f\"MLP hidden dimension: {model.cfg.d_mlp}\")\n",
    "print(f\"Vocabulary size: {model.cfg.d_vocab}\")\n",
    "print(f\"Context length: {model.cfg.n_ctx}\")\n",
    "print(f\"Head dimension: {model.cfg.d_head}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Architecture Numbers\n",
    "\n",
    "```\n",
    "GPT-2 Small\n",
    "├── Embedding: token (50257 → 768) + position (1024 → 768)\n",
    "├── 12 Transformer Blocks, each containing:\n",
    "│   ├── Layer Norm\n",
    "│   ├── Multi-Head Attention (12 heads, each 64-dim)\n",
    "│   │   ├── Q, K, V projections: 768 → 12×64\n",
    "│   │   └── Output projection: 12×64 → 768\n",
    "│   ├── Layer Norm\n",
    "│   └── MLP\n",
    "│       ├── Up projection: 768 → 3072\n",
    "│       ├── GELU activation\n",
    "│       └── Down projection: 3072 → 768\n",
    "└── Unembedding: 768 → 50257\n",
    "\n",
    "The residual stream (768-dim vector per token) flows through the entire network.\n",
    "Every attention head and MLP READS from it and WRITES back to it.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Residual Stream — Shared Memory\n",
    "\n",
    "The key insight from [Elhage et al. 2021](https://transformer-circuits.pub/2021/framework/index.html):\n",
    "\n",
    "> A transformer is not a sequence of layers that transform activations. It's a **residual stream** that components read from and write to.\n",
    "\n",
    "Every component (attention head, MLP) **adds** to the residual stream. The final residual stream is what gets turned into the next-token prediction.\n",
    "\n",
    "Let's actually see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a prompt through the model, caching ALL internal activations\n",
    "prompt = \"The capital of France is\"\n",
    "tokens = model.to_tokens(prompt)\n",
    "print(f\"Tokens: {[model.to_string(t) for t in tokens[0]]}\")\n",
    "print(f\"Token IDs: {tokens[0].tolist()}\")\n",
    "print(f\"Shape: {tokens.shape} (batch=1, seq_len={tokens.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with cache — this stores EVERY internal activation\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"  → (batch={logits.shape[0]}, seq_len={logits.shape[1]}, vocab={logits.shape[2]})\")\n",
    "print(f\"\\nCached activations: {len(cache)} tensors\")\n",
    "print(f\"\\nSome cache keys:\")\n",
    "for key in sorted(cache.keys())[:15]:\n",
    "    print(f\"  {key}: {cache[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The residual stream at each layer\n",
    "# This is THE central data structure — the 768-dim vector per token, evolving through the network\n",
    "\n",
    "# Get residual stream after each block\n",
    "residual_streams = []\n",
    "labels = []\n",
    "\n",
    "# After embedding (layer 0 input)\n",
    "residual_streams.append(cache[\"blocks.0.hook_resid_pre\"])\n",
    "labels.append(\"After Embed\")\n",
    "\n",
    "# After each layer\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    residual_streams.append(cache[f\"blocks.{layer}.hook_resid_post\"])\n",
    "    labels.append(f\"After Layer {layer}\")\n",
    "\n",
    "# Stack and look at the last token position (the prediction position)\n",
    "all_resid = torch.stack(residual_streams)[:, 0, -1, :]  # [layers, d_model]\n",
    "print(f\"Residual stream evolution: {all_resid.shape}\")\n",
    "print(f\"  → {all_resid.shape[0]} snapshots, each {all_resid.shape[1]}-dimensional\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: how much does each layer change the residual stream?\n",
    "norms = []\n",
    "for i in range(1, len(all_resid)):\n",
    "    delta = all_resid[i] - all_resid[i-1]\n",
    "    norms.append(delta.norm().item())\n",
    "\n",
    "fig = px.bar(\n",
    "    x=[f\"Layer {i}\" for i in range(12)],\n",
    "    y=norms,\n",
    "    title=\"How Much Each Layer Changes the Residual Stream\",\n",
    "    labels={\"x\": \"Layer\", \"y\": \"L2 Norm of Change\"},\n",
    "    template=\"plotly_dark\"\n",
    ")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Are Attention vs MLPs Contributing?\n",
    "\n",
    "Each layer has two components that write to the residual stream:\n",
    "1. **Attention** — moves information between token positions\n",
    "2. **MLP** — processes information at each position\n",
    "\n",
    "Let's see which one matters more at each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare attention vs MLP contributions at the last token\n",
    "attn_norms = []\n",
    "mlp_norms = []\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    attn_out = cache[f\"blocks.{layer}.hook_attn_out\"][0, -1, :]\n",
    "    mlp_out = cache[f\"blocks.{layer}.hook_mlp_out\"][0, -1, :]\n",
    "    attn_norms.append(attn_out.norm().item())\n",
    "    mlp_norms.append(mlp_out.norm().item())\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(name=\"Attention\", x=list(range(12)), y=attn_norms, marker_color=\"#6366f1\"))\n",
    "fig.add_trace(go.Bar(name=\"MLP\", x=list(range(12)), y=mlp_norms, marker_color=\"#f59e0b\"))\n",
    "fig.update_layout(\n",
    "    barmode=\"group\",\n",
    "    title=\"Attention vs MLP Contribution by Layer (last token)\",\n",
    "    xaxis_title=\"Layer\",\n",
    "    yaxis_title=\"L2 Norm of Output\",\n",
    "    template=\"plotly_dark\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attention Heads — Information Routing\n",
    "\n",
    "Each layer has 12 attention heads. Each head decides:\n",
    "- **What to attend to** (QK circuit) — which earlier tokens are relevant?\n",
    "- **What to move** (OV circuit) — what information to copy from those tokens?\n",
    "\n",
    "Attention patterns tell us *where* each head is looking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all attention patterns\n",
    "# Shape: [batch, n_heads, seq_len (query), seq_len (key)]\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "tokens = model.to_tokens(prompt)\n",
    "token_labels = [model.to_string(t) for t in tokens[0]]\n",
    "\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "# Show attention patterns for a few interesting heads\n",
    "def plot_attention(layer, head, title_extra=\"\"):\n",
    "    pattern = cache[f\"blocks.{layer}.attn.hook_pattern\"][0, head].detach().cpu().numpy()\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        pattern,\n",
    "        x=token_labels,\n",
    "        y=token_labels,\n",
    "        color_continuous_scale=\"Blues\",\n",
    "        title=f\"Layer {layer}, Head {head} — Attention Pattern {title_extra}\",\n",
    "        labels={\"x\": \"Key (attending TO)\", \"y\": \"Query (attending FROM)\", \"color\": \"Weight\"},\n",
    "        template=\"plotly_dark\"\n",
    "    )\n",
    "    fig.update_layout(width=500, height=500)\n",
    "    fig.show()\n",
    "\n",
    "# Plot a few heads\n",
    "plot_attention(0, 0, \"(early layer)\")\n",
    "plot_attention(5, 5, \"(middle layer)\")\n",
    "plot_attention(11, 10, \"(late layer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which heads attend most to the last token position?\n",
    "# This tells us which heads are most involved in predicting the next token\n",
    "\n",
    "last_token_attention = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    pattern = cache[f\"blocks.{layer}.attn.hook_pattern\"][0, :, -1, :]  # [heads, key_pos]\n",
    "    # What does the last query token attend to?\n",
    "    last_token_attention[layer] = pattern.sum(dim=-1).detach().cpu()  # just checking it sums to 1\n",
    "    # More interesting: where does the last token LOOK?\n",
    "    last_token_attention[layer] = pattern[:, -1].detach().cpu()  # attention on self\n",
    "\n",
    "# Better: for each head, what % of attention from last token goes to each position?\n",
    "fig_data = []\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    pattern = cache[f\"blocks.{layer}.attn.hook_pattern\"][0, :, -1, :].detach().cpu()  # [heads, key_pos]\n",
    "    for head in range(model.cfg.n_heads):\n",
    "        for pos, tok in enumerate(token_labels):\n",
    "            fig_data.append({\n",
    "                \"layer\": layer, \"head\": head,\n",
    "                \"position\": tok, \"pos_idx\": pos,\n",
    "                \"attention\": pattern[head, pos].item()\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(fig_data)\n",
    "\n",
    "# Heatmap: for each head, where does the LAST token attend?\n",
    "pivot = df.pivot_table(index=[\"layer\", \"head\"], columns=\"position\", values=\"attention\")\n",
    "# Reorder columns by position\n",
    "pivot = pivot[token_labels]\n",
    "\n",
    "fig = px.imshow(\n",
    "    pivot.values,\n",
    "    x=token_labels,\n",
    "    y=[f\"L{l}H{h}\" for l in range(12) for h in range(12)],\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    title=f'Where each head looks from the last position (predicting next token after \"{prompt}\")',\n",
    "    labels={\"x\": \"Attending to\", \"y\": \"Head\", \"color\": \"Attention\"},\n",
    "    template=\"plotly_dark\",\n",
    "    aspect=\"auto\"\n",
    ")\n",
    "fig.update_layout(height=1200)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Logit Lens — Watching the Model Think\n",
    "\n",
    "The **logit lens** is a beautifully simple technique:\n",
    "1. Take the residual stream at any intermediate layer\n",
    "2. Apply the final unembedding matrix (normally only applied at the end)\n",
    "3. See what the model would predict *at that point in processing*\n",
    "\n",
    "This reveals how the model's \"belief\" evolves layer by layer. Often the model \"knows\" the answer well before the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit lens: apply unembedding at each layer\n",
    "prompt = \"The capital of France is\"\n",
    "tokens = model.to_tokens(prompt)\n",
    "logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "# Get the model's final prediction\n",
    "final_probs = logits[0, -1].softmax(dim=-1)\n",
    "top_k_final = torch.topk(final_probs, 5)\n",
    "print(\"Final prediction (top 5):\")\n",
    "for i in range(5):\n",
    "    token_str = model.to_string(top_k_final.indices[i].item())\n",
    "    print(f\"  {token_str!r}: {top_k_final.values[i].item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now: what would the model predict at EACH layer?\n",
    "# (logit lens = apply unembedding to intermediate residual streams)\n",
    "\n",
    "# Track the top prediction and the correct answer's rank at each layer\n",
    "correct_token = model.to_tokens(\" Paris\")[0, 0]  # the token for \" Paris\"\n",
    "print(f\"Correct token: {model.to_string(correct_token)!r} (ID: {correct_token.item()})\")\n",
    "\n",
    "layer_predictions = []\n",
    "paris_probs = []\n",
    "paris_ranks = []\n",
    "\n",
    "for layer in range(model.cfg.n_layers + 1):  # +1 for after embedding\n",
    "    if layer == 0:\n",
    "        resid = cache[\"blocks.0.hook_resid_pre\"][0, -1]  # after embedding\n",
    "    else:\n",
    "        resid = cache[f\"blocks.{layer-1}.hook_resid_post\"][0, -1]  # after this layer\n",
    "    \n",
    "    # Apply layer norm + unembedding\n",
    "    normed = model.ln_final(resid)\n",
    "    layer_logits = model.unembed(normed.unsqueeze(0).unsqueeze(0))[0, 0]\n",
    "    probs = layer_logits.softmax(dim=-1)\n",
    "    \n",
    "    # Top prediction\n",
    "    top_token = layer_logits.argmax().item()\n",
    "    top_prob = probs[top_token].item()\n",
    "    \n",
    "    # Where is \"Paris\"?\n",
    "    paris_prob = probs[correct_token].item()\n",
    "    paris_rank = (probs > paris_prob).sum().item() + 1\n",
    "    \n",
    "    layer_name = \"Embed\" if layer == 0 else f\"Layer {layer-1}\"\n",
    "    top_str = model.to_string(top_token)\n",
    "    layer_predictions.append({\n",
    "        \"layer\": layer_name,\n",
    "        \"top_prediction\": top_str,\n",
    "        \"top_prob\": top_prob,\n",
    "        \"paris_prob\": paris_prob,\n",
    "        \"paris_rank\": paris_rank\n",
    "    })\n",
    "    paris_probs.append(paris_prob)\n",
    "    paris_ranks.append(paris_rank)\n",
    "    \n",
    "    print(f\"{layer_name:>10}: top={top_str!r:>12} ({top_prob:.3f}) | 'Paris' rank={paris_rank:>5}, prob={paris_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the logit lens\n",
    "layer_names = [d[\"layer\"] for d in layer_predictions]\n",
    "\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True,\n",
    "                    subplot_titles=[\"Probability of 'Paris' at Each Layer\",\n",
    "                                   \"Rank of 'Paris' at Each Layer\"])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=layer_names, y=paris_probs, mode=\"lines+markers\",\n",
    "               marker=dict(size=10, color=\"#6366f1\"), name=\"Probability\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=layer_names, y=paris_ranks, mode=\"lines+markers\",\n",
    "               marker=dict(size=10, color=\"#f59e0b\"), name=\"Rank\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template=\"plotly_dark\",\n",
    "    title=f'Logit Lens: How \"{prompt}\" → \"Paris\" builds up',\n",
    "    height=600,\n",
    "    showlegend=False\n",
    ")\n",
    "fig.update_yaxes(title_text=\"Probability\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Rank (lower=better)\", row=2, col=1, autorange=\"reversed\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Different Prompts\n",
    "\n",
    "Experiment! Change the prompt and see how the logit lens behaves differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own!\n",
    "def logit_lens(prompt, target_token=None):\n",
    "    \"\"\"Run the logit lens on any prompt. If target_token is None, uses the model's top final prediction.\"\"\"\n",
    "    tokens = model.to_tokens(prompt)\n",
    "    logits, cache = model.run_with_cache(tokens)\n",
    "    \n",
    "    if target_token is None:\n",
    "        target_id = logits[0, -1].argmax().item()\n",
    "    else:\n",
    "        target_id = model.to_tokens(target_token)[0, 0].item()\n",
    "    \n",
    "    target_str = model.to_string(target_id)\n",
    "    probs = []\n",
    "    layer_names = []\n",
    "    \n",
    "    for layer in range(model.cfg.n_layers + 1):\n",
    "        if layer == 0:\n",
    "            resid = cache[\"blocks.0.hook_resid_pre\"][0, -1]\n",
    "        else:\n",
    "            resid = cache[f\"blocks.{layer-1}.hook_resid_post\"][0, -1]\n",
    "        \n",
    "        normed = model.ln_final(resid)\n",
    "        layer_logits = model.unembed(normed.unsqueeze(0).unsqueeze(0))[0, 0]\n",
    "        prob = layer_logits.softmax(dim=-1)[target_id].item()\n",
    "        probs.append(prob)\n",
    "        layer_names.append(\"Embed\" if layer == 0 else f\"L{layer-1}\")\n",
    "    \n",
    "    fig = px.line(\n",
    "        x=layer_names, y=probs,\n",
    "        title=f'Logit Lens: \"{prompt}\" → \"{target_str}\"',\n",
    "        labels={\"x\": \"Layer\", \"y\": f\"P('{target_str}')\"},\n",
    "        template=\"plotly_dark\",\n",
    "        markers=True\n",
    "    )\n",
    "    fig.show()\n",
    "    return probs\n",
    "\n",
    "# Examples to try:\n",
    "logit_lens(\"The Eiffel Tower is located in\")\n",
    "# logit_lens(\"1 + 1 =\", \" 2\")\n",
    "# logit_lens(\"Barack Obama was the 44th\")\n",
    "# logit_lens(\"def fibonacci(n):\\n    if n <= 1:\\n        return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Your First Causal Intervention: Ablation\n",
    "\n",
    "Everything so far was *observational* — looking at what activates. But the real power of mech interp is **causal** — what happens if we *change* things?\n",
    "\n",
    "Simplest intervention: **ablation**. Remove a component and see what breaks.\n",
    "\n",
    "Question: which attention heads matter most for predicting \"Paris\" after \"The capital of France is\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablation study: zero out each attention head one at a time, \n",
    "# measure how much the probability of \"Paris\" drops\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "tokens = model.to_tokens(prompt)\n",
    "target_token = model.to_tokens(\" Paris\")[0, 0]\n",
    "\n",
    "# Baseline: normal prediction\n",
    "clean_logits = model(tokens)\n",
    "clean_prob = clean_logits[0, -1].softmax(dim=-1)[target_token].item()\n",
    "print(f\"Baseline P(' Paris') = {clean_prob:.4f}\")\n",
    "\n",
    "# Ablate each head and measure the effect\n",
    "effects = torch.zeros(model.cfg.n_layers, model.cfg.n_heads)\n",
    "\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for head in range(model.cfg.n_heads):\n",
    "        # Hook that zeros out this specific head's output\n",
    "        def ablation_hook(value, hook, head_idx=head):\n",
    "            value[:, :, head_idx, :] = 0\n",
    "            return value\n",
    "        \n",
    "        # Run with the hook\n",
    "        ablated_logits = model.run_with_hooks(\n",
    "            tokens,\n",
    "            fwd_hooks=[(f\"blocks.{layer}.attn.hook_result\", ablation_hook)]\n",
    "        )\n",
    "        ablated_prob = ablated_logits[0, -1].softmax(dim=-1)[target_token].item()\n",
    "        effects[layer, head] = clean_prob - ablated_prob  # positive = head helped\n",
    "\n",
    "print(f\"\\nDone! Effects range: [{effects.min():.4f}, {effects.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: which heads matter?\n",
    "fig = px.imshow(\n",
    "    effects.detach().cpu().numpy(),\n",
    "    x=[f\"Head {h}\" for h in range(12)],\n",
    "    y=[f\"Layer {l}\" for l in range(12)],\n",
    "    color_continuous_scale=\"RdBu\",\n",
    "    color_continuous_midpoint=0,\n",
    "    title=f'Head Ablation: Effect on P(\" Paris\") — red=helps, blue=hurts',\n",
    "    labels={\"color\": \"Δ P(' Paris')\"},\n",
    "    template=\"plotly_dark\"\n",
    ")\n",
    "fig.update_layout(width=700, height=600)\n",
    "fig.show()\n",
    "\n",
    "# Top 5 most important heads\n",
    "flat = effects.flatten()\n",
    "top_indices = flat.abs().topk(5).indices\n",
    "print(\"\\nTop 5 most important heads for predicting 'Paris':\")\n",
    "for idx in top_indices:\n",
    "    layer = idx.item() // 12\n",
    "    head = idx.item() % 12\n",
    "    eff = effects[layer, head].item()\n",
    "    direction = \"helps\" if eff > 0 else \"hurts\"\n",
    "    print(f\"  Layer {layer}, Head {head}: Δ = {eff:+.4f} ({direction})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What Did We Just Do?\n",
    "\n",
    "Let's take stock:\n",
    "\n",
    "1. **Loaded a real model** and accessed all internal activations\n",
    "2. **Saw the residual stream** — the model's shared memory, evolving through layers\n",
    "3. **Measured** which components (attention vs MLP) contribute most at each layer\n",
    "4. **Visualized attention patterns** — where each head looks\n",
    "5. **Logit lens** — watched the model's prediction build up layer by layer\n",
    "6. **Causal ablation** — found which specific heads are responsible for the prediction\n",
    "\n",
    "### Key Intuitions to Take Away\n",
    "\n",
    "- The **residual stream** is the backbone. Components don't talk to each other directly — they communicate through it.\n",
    "- **Attention heads** route information between positions. **MLPs** process information at each position.\n",
    "- The model often \"knows\" the answer in the **middle layers** — late layers are refining, not discovering.\n",
    "- **Ablation** is the simplest causal tool: remove → observe → attribute.\n",
    "- Even in a \"simple\" task like capital prediction, **many heads** participate, and some heads actually hurt the answer.\n",
    "\n",
    "### What's Next\n",
    "\n",
    "Now that you can see inside the model, the next question is: **why are these activations so hard to interpret?**\n",
    "\n",
    "That leads us to **superposition** and **sparse autoencoders** — the core of modern mech interp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLAYGROUND: Try your own experiments below!\n",
    "# \n",
    "# Ideas:\n",
    "# - Try different prompts with the logit_lens() function\n",
    "# - Look at attention patterns for longer prompts\n",
    "# - Ablate MLP layers instead of attention heads\n",
    "# - Compare how the model handles \"The capital of Germany is\" vs \"The capital of France is\"\n",
    "# - Find which head is responsible for copying patterns (induction heads)\n",
    "#\n",
    "# The model and cache are already loaded — just start experimenting!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Interp Research (3.12)",
   "language": "python",
   "name": "interp"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
